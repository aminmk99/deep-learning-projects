# Regularization in Machine Learning  

## What is Regularization?  
Regularization is a technique used in machine learning to prevent **overfitting** by adding a penalty to the loss function. Overfitting occurs when a model learns the noise in the training data instead of the underlying pattern, leading to poor generalization on new data.

## Why is Regularization Important?  
- Prevents overfitting by discouraging complex models.
- Improves model generalization to unseen data.
- Helps in dealing with multicollinearity (in linear regression).

## Types of Regularization  
### 1. L1 Regularization (Lasso Regression)  
- Adds the **absolute** value of coefficients (`|w|`) as a penalty term.  
- Encourages sparsity by forcing some coefficients to become zero (feature selection).  
- Cost function:  
  \[
  J(w) = Loss + \lambda \sum |w_i|
  \]

### 2. L2 Regularization (Ridge Regression)  
- Adds the **squared** value of coefficients (`w¬≤`) as a penalty.  
- Shrinks coefficients towards zero but does not make them exactly zero.  
- Cost function:  
  \[
  J(w) = Loss + \lambda \sum w_i^2
  \]

### 3. Elastic Net  
- A combination of L1 and L2 regularization.  
- Useful when there are correlated features.  
- Cost function:  
  \[
  J(w) = Loss + \lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2
  \]

## Regularization in Neural Networks  
Regularization in deep learning is achieved using:
- **L1/L2 weight penalties** (applied to network weights).
- **Dropout** (randomly disabling neurons to prevent co-adaptation).
- **Batch Normalization** (normalizing activations to stabilize learning).
- **Early Stopping** (stopping training when validation loss stops improving).

## Code Example (L2 Regularization in TensorFlow/Keras)
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l2

model = Sequential([
    Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)),
    Dense(1, activation='sigmoid')
])
```
# When to Use L1, L2, and Elastic Net Regularization

## 1. L1 Regularization (Lasso) ‚Äì When Feature Selection is Important  

‚úÖ **Use L1 Regularization when:**  
- You **suspect that many features are irrelevant** or redundant.  
- You need **automatic feature selection** because L1 regularization **shrinks some coefficients to zero**, effectively removing those features.  
- Your dataset is **high-dimensional** (many features, but not all are useful).  

üöÄ **Example Use Cases:**  
- **Sparse models** where only a few features contribute significantly (e.g., text classification, genetics).  
- **Reducing complexity** in models with many irrelevant features.  

‚ö†Ô∏è **Limitations:**  
- If features are correlated, L1 tends to select only **one** of them and ignore the rest, which might not be ideal in some cases.  
- It can be **unstable** if there are multiple equally important features.  

---

## 2. L2 Regularization (Ridge) ‚Äì When All Features Are Important  

‚úÖ **Use L2 Regularization when:**  
- You believe **all features contribute to the output**, but their impact should be controlled to avoid overfitting.  
- You have **multicollinearity** (highly correlated features), since L2 keeps all features but reduces their influence proportionally.  
- You want **smooth and stable weight updates** during training (L2 doesn't force weights to zero).  

üöÄ **Example Use Cases:**  
- **Regression problems** where all independent variables play a role.  
- **Deep learning** to keep weights small but nonzero.  

‚ö†Ô∏è **Limitations:**  
- L2 does **not perform feature selection** (it reduces weights but doesn‚Äôt remove them).  
- If you have too many irrelevant features, L2 alone may not help.  

---

## 3. Elastic Net ‚Äì When You Want the Best of Both Worlds  

‚úÖ **Use Elastic Net when:**  
- You suspect **some features are irrelevant**, but you also want to **keep correlated features** together instead of discarding one like L1 does.  
- You need **feature selection (L1) but also want to avoid instability** when features are correlated (L2 helps with that).  
- Your dataset has **many features, some of which are highly correlated, and some are redundant**.  

üöÄ **Example Use Cases:**  
- **High-dimensional datasets** with both relevant and irrelevant features (e.g., gene expression data, financial models).  
- **Sparse data problems** where pure L1 is too aggressive in feature elimination.  

‚ö†Ô∏è **Limitations:**  
- You need to **tune two hyperparameters** (`Œª1` for L1 and `Œª2` for L2), which makes Elastic Net slightly more complex.  

---

## Summary Table: L1 vs L2 vs Elastic Net  

| Regularization | When to Use? | Effect on Weights | Handles Correlated Features? | Feature Selection? |
|--------------|--------------|-------------------|----------------------|----------------|
| **L1 (Lasso)** | Many features, but only a few are important | Shrinks some weights to exactly **zero** | ‚ùå No, picks one and drops others | ‚úÖ Yes |
| **L2 (Ridge)** | All features contribute, but should be controlled | Shrinks all weights **closer to zero**, but **not zero** | ‚úÖ Yes, keeps all | ‚ùå No |
| **Elastic Net** | Many features, some redundant, some important | Shrinks weights + selects important features | ‚úÖ Yes, better than L1 alone | ‚úÖ Yes |

---

## Final Thoughts: Which One Should You Use?  
- If you **expect only a few features to be important**, use **L1 (Lasso)**.  
- If you **think all features matter**, but they should be small, use **L2 (Ridge)**.  
- If you **want both feature selection and stability**, use **Elastic Net**.  
